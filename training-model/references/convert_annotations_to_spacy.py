from concurrent.futures import Future, ProcessPoolExecutor
import hashlib
import io
import logging
from pathlib import Path
from random import choices, random, seed
import subprocess
import time
from typing import List, Optional, Tuple
from xml.sax.saxutils import unescape

from lxml import etree
import numpy
from numpy.random import randint
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split
import spacy
from spacy.tokens import Doc, DocBin, Span
from tqdm import tqdm
import typer

from bib_tokenizers import create_references_tokenizer
from csl_client import base_url, make_bibliography, styles_list
from lxml_iter_tree import annotations, get_text, parse
from schema import spankey_sentence_start, tag_sentence_start, tags_ent, tags_span

# less is better :) Consider download mo files from CrossRef
NUM_STYLES_FOR_DOC = 1
# usually >1 if NUM_STYLES_FOR_DOC>1
DOWNSAMPE_RATIO = 1

# it is relatively small because it is limited by GPU RAM: Spacy seems to load all dev examples at once.
DEV_SIZE = 0.02

SEED = 42

# sample styles:
# 1. choice a dir with CSLs given weights
# 2. choice a number of styles from the dir at random
# cslSmallWithTags contains popular styles selected by hand
# cslWithTags contains all styles from the CSL repo
styles_dirs = ["cslSmallWithTags", "cslWithTags"]
styles_dir_weights = [0.8, 0.2]
assert len(styles_dirs), len(styles_dir_weights)

log = logging.getLogger(__name__)
blank_nlp = spacy.blank("en")
blank_nlp.tokenizer = create_references_tokenizer()(blank_nlp)


def references_to_spacy(
    references,
    style: str,
    references_per_doc_range=(5, 10),
    fname: Optional[str] = None,
) -> List[Optional[Doc]]:
    """
    convert list of references (aka bibliography section) into Spacy annotated docs
    Annotations will include:
        - START_SENTENCE for each first token of a reference from the list
        - doc.ents for NER (only non-overlapped spans, TBD what is included)
        - [overlapped] doc.ents["bib"] will contain all annotated spans

    Args:
        references - list of references with annotation as they were generated by CSL proceesor
        style - string, style name used by CSL processor to generate entities. It will be stored in doc.user_data
        references_per_doc_range - used to split up large bibliographies into smaller docs.
                                   Some randomness allows to avoid 'split bias',
                                   when all chunks start from the same citation numbers, e.g., 1, 11, 21, etc

    TBD: check that sha("".join(references)) is uniq, if different styles produced the same output ...
    """
    references_per_doc = randint(*references_per_doc_range)

    parts = [
        arr.tolist()
        for arr in numpy.array_split(
            numpy.array(references), len(references) // references_per_doc + 1
        )
    ]
    return [
        references_to_spacy_doc(part, style, fname, i) for i, part in enumerate(parts)
    ]


def normalize_multiline(text) -> str:
    """
    In [7]: text = "I \t like   \n   cheese."
    In [8]: normalize_multiline(text)
    Out[8]: 'I \t like cheese.'
    """
    f = io.StringIO(text)
    lines = (line for line in f)
    return " ".join((line.strip() for line in lines if line.strip()))


def references_to_spacy_doc(
    references, style: str, fname: Optional[str] = None, part=0
) -> Optional[Doc]:
    norm_references = (normalize_multiline(ref) for ref in references)
    # don't miss the space between already normalized lines:
    xml = f"<references><bib>{' </bib><bib>'.join(norm_references)}</bib></references>"
    try:
        root = parse(xml)
    except etree.XMLSyntaxError as e:
        log.exception("%s: cannot parse %s", e, references)
        return

    # create doc from text
    text = get_text(root)
    doc = blank_nlp(text)

    # add annotations: they are overlapped spans
    def alignment_mode(tag):
        # include doi prefix such as https://doi.org/
        return "expand" if tag == "doi" else "contract"

    spans = [
        doc.char_span(start, end, label=tag, alignment_mode=alignment_mode(tag))
        for tag, start, end in annotations(root, tags_to_be_included=tags_span)
    ]
    # doc.char_span can return None if character indices can't be snaped to token boundaries
    spans = [span for span in spans if span is not None]

    # store annotations as possible overlapped spans
    doc.spans["bib"] = spans

    # add bib item boundaries as spans.
    # rationale: predictiong entire bib item span require generation a lot of candidates:
    # something like max_bib_len*doc.len, that could be expensive
    # instead we can predict just one-token spans
    # bibs = [span for span in spans if span.label_ == tag_sentence_start]
    # _len = 2
    # bib_boundaries = [
    #     Span(doc, bib.start, bib.start + _len, "bib_start") for bib in bibs
    # ] + [Span(doc, bib.end - _len, bib.end, "bib_end") for bib in bibs]
    # doc.spans["bib_boundaries"] = bib_boundaries

    # add not-overlapped spans as doc.ents for NER
    ents = []
    for ent in [span for span in spans if span.label_ in tags_ent]:

        def is_bounding(t):
            return not (t.is_punct or t.is_space)

        # "If you're training a named entity recognizer, also make sure that none of your
        #       annotated entity spans have leading or trailing whitespace or punctuation"
        while len(ent) > 1 and not is_bounding(ent[0]):
            ent = Span(doc, ent.start + 1, ent.end, ent.label)
        while len(ent) > 1 and not is_bounding(ent[-1]):
            ent = Span(doc, ent.start, ent.end - 1, ent.label)

        if is_bounding(ent[0]) and is_bounding(ent[-1]):
            ents.append(ent)
    doc.set_ents(ents)

    # set is_sent_start for https://spacy.io/api/sentencerecognizer
    sent_start_spans = []
    for span in spans:
        if span.label_ == tag_sentence_start:
            span[0].is_sent_start = True

            # experimental: add 'sent start span': it is easiest way to get scores when inference
            sent_start_spans.append(
                Span(doc, span.start, span.start + 1, tag_sentence_start)
            )
    doc.spans[spankey_sentence_start] = sent_start_spans  # "bib, bib,... bib ... "

    # from pprint import pprint
    # pprint([(span.label_, span.text) for span in spans if span])

    doc.user_data = {"title": f"{fname}[{part}] - {style}", "bib": {"style": style}}
    return doc


def convert(
    crossref_files: List[Path],
    docbin_train_path: Path,
    docbin_dev_path: Path,
    csl_http_port=3000,
    csl_processor_path=f"{Path(__file__).parent}/../../dataset-creation/processManuscript.js",
):

    csl_processor_path = Path(csl_processor_path).resolve()
    if not csl_processor_path.is_file():
        print(f"{csl_processor_path} does not exist")
        exit(1)
    csl_processor_cwd = csl_processor_path.parent

    with subprocess.Popen(
        ["node", csl_processor_path, str(csl_http_port)],
        cwd=csl_processor_cwd,
    ) as p:
        print(f"CSL processor: {csl_processor_path}, pid: {p.pid}")
        time.sleep(1)
        styles: List[Tuple[str, int]] = [
            (dir, len(styles_list(url=base_url(csl_http_port), styles_dir=dir)))
            for dir in styles_dirs
        ]
        print(f"Sampling styles from:", styles, "with dir weights:", styles_dir_weights)

        digests = set()
        all_docs = []  # created by applying CSL to crossref_files
        for f in tqdm(crossref_files, desc=docbin_train_path.name):
            styles_dir, num_styles = choices(styles, weights=styles_dir_weights)[0]
            try:
                bibliographies = make_bibliography(
                    f,
                    randint(0, num_styles, NUM_STYLES_FOR_DOC).tolist(),
                    url=base_url(csl_http_port),
                    styles_dir=styles_dir,
                )
                for bibliography in bibliographies:
                    if "references" not in bibliography:
                        # print("a problem for style ", bibliography["style"])
                        continue
                    references = bibliography["references"]
                    style = bibliography["style"]

                    # check digest if two styles produce the same string representation of the same crossref item
                    _references = []
                    for ref in references:
                        digest = hashlib.md5(ref.encode("utf-8")).digest()
                        if digest not in digests:
                            digests.add(digest)
                            _references.append(ref)
                        # else:
                        #     print("duplicate bib item: %s, style: %s", ref, style)
                    references = _references
                    if not references:
                        continue

                    docs = references_to_spacy(references, style, fname=f.name)

                    # downsample
                    if DOWNSAMPE_RATIO > 1 and len(docs) // DOWNSAMPE_RATIO > 0:
                        docs = numpy.random.choice(
                            numpy.array(docs, dtype=numpy.dtype("object")),
                            len(docs) // DOWNSAMPE_RATIO,
                            replace=False,
                        )

                    all_docs.extend(docs)

            except:
                log.exception("An exception while processing %s", f)

        def write_docs(docs, path):
            db = DocBin(store_user_data=True)
            for doc in docs:
                if doc:
                    db.add(doc)
                else:
                    log.warning("Can't parse a part of %s", f)
            db.to_disk(path)

        docs_train, docs_dev = train_test_split(all_docs, test_size=DEV_SIZE)
        write_docs(docs_train, docbin_train_path)
        write_docs(docs_dev, docbin_dev_path)

        p.terminate()

    return docbin_train_path, docbin_dev_path


def main(
    crossref_dir: Path,
    train_dir: Path = Path("train.ref"),
    dev_dir: Path = Path("dev.ref"),
    parts: int = 100,
    parallel: int = 2,
    csl_processor_path=f"{Path(__file__).parent}/../../dataset-creation/processManuscript.js",
):
    """
    Sends crossref files to CSL processor and convert rendered annotated biblioraphies into Spacy DocBin files
    Dependency:
        node processManuscript.js
        needs to be started, for additional details see dataset-creation/README.md
    Args:
        crossref_dir: a directory with files downloaded from crossref, see dataset-creation/crossref/crossrefDownload.py
        output_dir: where to put *.docbin files
        parts: how many docbin files to be generated
        parallel: how manu processes to use. You might want to launch more
                  than one dataset-creation/processManuscript.js node process, i.e. in k8s cluster,
                  if you need high parallelism
    """
    for dir in [train_dir, dev_dir]:
        Path(dir).mkdir(parents=True, exist_ok=True)

    input_files = [path for path in crossref_dir.glob("**/*.json")]
    input_files_parts = [
        arr.tolist() for arr in numpy.array_split(numpy.array(input_files), parts)
    ]
    futures: List[Future] = []
    if parallel > 0:
        with ProcessPoolExecutor(parallel) as e:
            for i, input_files_part in enumerate(input_files_parts):
                futures.append(
                    e.submit(
                        convert,
                        input_files_part,
                        train_dir / f"references.{i}.spacy",
                        dev_dir / f"references.{i}.spacy",
                        3000 + i,
                        csl_processor_path,
                    )
                )
            for f in futures:
                print("convert:done:", f.result())
    else:
        # useful for debugging with ipython.embed()
        for i, input_files_part in enumerate(input_files_parts):
            result = convert(
                input_files_part,
                train_dir / f"references.{i}.spacy",
                dev_dir / f"references.{i}.spacy",
                3000 + i,
                csl_processor_path,
            )
            print("convert:done:", result)


if __name__ == "__main__":
    seed(SEED)
    numpy.random.seed(SEED)
    typer.run(main)

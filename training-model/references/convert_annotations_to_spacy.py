from concurrent.futures import Future, ProcessPoolExecutor
import logging
from pathlib import Path
from typing import List, Optional
from tqdm import tqdm
from lxml import etree
from numpy.random import randint
import spacy
from spacy.tokens import Doc, DocBin, Span
import typer
import numpy

from csl_client import make_bibliography, styles_list
from lxml_iter_tree import annotations
from schema import tag_sentence_start, tags_span, tags_ent

NUM_STYLES_FOR_DOC = 10
DOWNSAMPE_RATIO = 10

log = logging.getLogger(__name__)
blank_nlp = spacy.blank("en")


def references_to_spacy(
    references,
    style: str,
    references_per_doc_range=(5, 10),
    fname: Optional[str] = None,
) -> List[Optional[Doc]]:
    """
    convert list of references (aka bibliography section) into Spacy annotated docs
    Annotations will include:
        - START_SENTENCE for each first token of a reference from the list
        - doc.ents for NER (only non-overlapped spans, TBD what is included)
        - [overlapped] doc.ents["bib"] will contain all annotated spans

    Args:
        references - list of references with annotation as they were generated by CSL proceesor
        style - string, style name used by CSL processor to generate entities. It will be stored in doc.user_data
        references_per_doc_range - used to split up large bibliographies into smaller docs.
                                   Some randomness allows to avoid 'split bias',
                                   when all chunks start from the same citation numbers, e.g., 1, 11, 21, etc

    TBD: check that sha("".join(references)) is uniq, if different styles produced the same output ...
    """
    references_per_doc = randint(*references_per_doc_range)

    parts = [
        arr.tolist()
        for arr in numpy.array_split(
            numpy.array(references), len(references) // references_per_doc + 1
        )
    ]
    return [
        references_to_spacy_doc(part, style, fname, i) for i, part in enumerate(parts)
    ]


def references_to_spacy_doc(
    references, style: str, fname: Optional[str] = None, part=0
) -> Optional[Doc]:
    xml = f"<references><bib>{'</bib><bib>'.join(references)}</bib></references>"
    try:
        parser = etree.HTMLParser()
        root = etree.fromstring(xml, parser)
    except etree.XMLSyntaxError as e:
        log.exception("cannot parse %s", references)
        return
    # create doc from text
    doc = blank_nlp("".join(root.itertext()))
    # add annotations: they are overlapped spans

    spans = [
        doc.char_span(start, end, label=tag, alignment_mode="contract")
        for tag, start, end in annotations(root, tags_to_be_included=tags_span)
    ]
    # doc.char_span can return None if character indices can't be snaped to token boundaries
    spans = [span for span in spans if span is not None]

    # add annotations as possible overlapped spans for SpanCategorizer
    doc.spans["bib"] = spans

    # add not-overlapped spans as doc.ents for NER
    ents = []
    for ent in [span for span in spans if span.label_ in tags_ent]:

        def is_bounding(t):
            return not (t.is_punct or t.is_space)

        # "If you're training a named entity recognizer, also make sure that none of your
        #       annotated entity spans have leading or trailing whitespace or punctuation"
        while len(ent) > 1 and not is_bounding(ent[0]):
            ent = Span(doc, ent.start + 1, ent.end, ent.label)
        while len(ent) > 1 and not is_bounding(ent[-1]):
            ent = Span(doc, ent.start, ent.end - 1, ent.label)

        if is_bounding(ent[0]) and is_bounding(ent[-1]):
            ents.append(ent)
    doc.set_ents(ents)

    # set is_sent_start for https://spacy.io/api/sentencerecognizer
    for span in spans:
        if span.label_ == tag_sentence_start:
            span[0].is_sent_start = True

    # from pprint import pprint
    # pprint([(span.label_, span.text) for span in spans if span])

    doc.user_data = {"title": f"{fname}[{part}] - {style}", "bib": {"style": style}}
    return doc


def convert(crossref_files: List[Path], docbin_path: Path):
    styles: List[str] = styles_list()
    len_styles = len(styles)
    print(f"CSL processor supports {len_styles} styles")

    db = DocBin(store_user_data=True)
    for f in tqdm(crossref_files, desc=docbin_path.name):
        try:
            bibliographies = make_bibliography(
                f, randint(0, len_styles, NUM_STYLES_FOR_DOC).tolist()
            )
            for bibliography in bibliographies:
                if "references" not in bibliography:
                    print("a problem for style ", bibliography["style"])
                    continue
                references = bibliography["references"]
                style = bibliography["style"]
                docs = references_to_spacy(references, style, fname=f.name)

                # downsample
                if len(docs) // DOWNSAMPE_RATIO > 0:
                    docs = numpy.random.choice(
                        numpy.array(docs, dtype=numpy.dtype("object")),
                        len(docs) // DOWNSAMPE_RATIO,
                        replace=False,
                    )

                for doc in docs:
                    if doc:
                        db.add(doc)
                    else:
                        log.warning("Can't parse a part of %s", f)
        except:
            log.exception("An exception while processing %s", f)

    db.to_disk(docbin_path)
    return docbin_path


def main(
    crossref_dir: Path,
    output_dir: Path = Path("train.ref"),
    parts: int = 100,
    parallel: int = 2,
):
    """
    Sends crossref files to CSL processor and convert rendered annotated biblioraphies into Spacy DocBin files
    Dependency:
        node processManuscript.js
        needs to be started, for additional details see dataset-creation/README.md
    Args:
        crossref_dir: a directory with files downloaded from crossref, see dataset-creation/crossref/crossrefDownload.py
        output_dir: where to put *.docbin files
        parts: how many docbin files to be generated
        parallel: how manu processes to use. You might want to launch more
                  than one dataset-creation/processManuscript.js node process, i.e. in k8s cluster,
                  if you need high parallelism
    """

    input_files = [path for path in crossref_dir.glob("**/*.json")]
    input_files_parts = [
        arr.tolist() for arr in numpy.array_split(numpy.array(input_files), parts)
    ]
    futures: List[Future] = []
    with ProcessPoolExecutor(parallel) as e:
        for i, input_files_part in enumerate(input_files_parts):
            futures.append(
                e.submit(
                    convert, input_files_part, output_dir / f"references.{i}.spacy"
                )
            )
        for f in futures:
            print("convert:done:", f.result())


if __name__ == "__main__":
    typer.run(main)

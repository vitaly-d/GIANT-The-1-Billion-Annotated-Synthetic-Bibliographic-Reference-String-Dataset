from concurrent.futures import Future, ProcessPoolExecutor
import hashlib
import logging
from pathlib import Path
import subprocess
import time
from typing import List, Optional

from lxml import etree
import numpy
from numpy.random import randint
import spacy
from spacy.tokens import Doc, DocBin, Span
from tqdm import tqdm
import typer

from bib_tokenizers import create_references_tokenizer
from csl_client import base_url, make_bibliography, styles_list
from lxml_iter_tree import annotations
from schema import (
    tag_sentence_start,
    tags_ent,
    tags_span,
    spankey_sentence_start,
    # token_bib_end,
    # token_bib_start,
    # token_other,
    # token_tags,
)

NUM_STYLES_FOR_DOC = 10
DOWNSAMPE_RATIO = 5

log = logging.getLogger(__name__)
blank_nlp = spacy.blank("en")
blank_nlp.tokenizer = create_references_tokenizer()(blank_nlp)


def references_to_spacy(
    references,
    style: str,
    references_per_doc_range=(5, 10),
    fname: Optional[str] = None,
) -> List[Optional[Doc]]:
    """
    convert list of references (aka bibliography section) into Spacy annotated docs
    Annotations will include:
        - START_SENTENCE for each first token of a reference from the list
        - doc.ents for NER (only non-overlapped spans, TBD what is included)
        - [overlapped] doc.ents["bib"] will contain all annotated spans

    Args:
        references - list of references with annotation as they were generated by CSL proceesor
        style - string, style name used by CSL processor to generate entities. It will be stored in doc.user_data
        references_per_doc_range - used to split up large bibliographies into smaller docs.
                                   Some randomness allows to avoid 'split bias',
                                   when all chunks start from the same citation numbers, e.g., 1, 11, 21, etc

    TBD: check that sha("".join(references)) is uniq, if different styles produced the same output ...
    """
    references_per_doc = randint(*references_per_doc_range)

    parts = [
        arr.tolist()
        for arr in numpy.array_split(
            numpy.array(references), len(references) // references_per_doc + 1
        )
    ]
    return [
        references_to_spacy_doc(part, style, fname, i) for i, part in enumerate(parts)
    ]


def references_to_spacy_doc(
    references, style: str, fname: Optional[str] = None, part=0
) -> Optional[Doc]:
    xml = f"<references><bib>{'</bib><bib>'.join(references)}</bib></references>"
    try:
        parser = etree.HTMLParser()
        root = etree.fromstring(xml, parser)
    except etree.XMLSyntaxError as e:
        log.exception("cannot parse %s", references)
        return
    # create doc from text
    text = "".join(root.itertext())
    text = text.replace("\n", " ")
    doc = blank_nlp(text)
    # add annotations: they are overlapped spans

    spans = [
        doc.char_span(start, end, label=tag, alignment_mode="contract")
        for tag, start, end in annotations(root, tags_to_be_included=tags_span)
    ]
    # doc.char_span can return None if character indices can't be snaped to token boundaries
    spans = [span for span in spans if span is not None]

    # store annotations as possible overlapped spans
    doc.spans["bib"] = spans

    # add bib item boundaries as spans.
    # rationale: predictiong entire bib item span require generation a lot of candidates:
    # something like max_bib_len*doc.len, that could be expensive
    # instead we can predict just one-token spans
    # bibs = [span for span in spans if span.label_ == tag_sentence_start]
    # _len = 2
    # bib_boundaries = [
    #     Span(doc, bib.start, bib.start + _len, "bib_start") for bib in bibs
    # ] + [Span(doc, bib.end - _len, bib.end, "bib_end") for bib in bibs]
    # doc.spans["bib_boundaries"] = bib_boundaries

    # add not-overlapped spans as doc.ents for NER
    ents = []
    for ent in [span for span in spans if span.label_ in tags_ent]:

        def is_bounding(t):
            return not (t.is_punct or t.is_space)

        # "If you're training a named entity recognizer, also make sure that none of your
        #       annotated entity spans have leading or trailing whitespace or punctuation"
        while len(ent) > 1 and not is_bounding(ent[0]):
            ent = Span(doc, ent.start + 1, ent.end, ent.label)
        while len(ent) > 1 and not is_bounding(ent[-1]):
            ent = Span(doc, ent.start, ent.end - 1, ent.label)

        if is_bounding(ent[0]) and is_bounding(ent[-1]):
            ents.append(ent)
    doc.set_ents(ents)

    # set is_sent_start for https://spacy.io/api/sentencerecognizer
    sent_start_spans = []
    for span in spans:
        if span.label_ == tag_sentence_start:
            span[0].is_sent_start = True

            # experimental: add 'sent start span': it is easiest way to get scores when inference
            sent_start_spans.append(
                Span(doc, span.start, span.start + 1, tag_sentence_start)
            )
    doc.spans[spankey_sentence_start] = sent_start_spans  # "bib, bib,... bib ... "

    # from pprint import pprint
    # pprint([(span.label_, span.text) for span in spans if span])

    doc.user_data = {"title": f"{fname}[{part}] - {style}", "bib": {"style": style}}
    return doc


def convert(
    crossref_files: List[Path],
    docbin_path: Path,
    csl_http_port=3000,
    csl_processor_path=f"{Path(__file__).parent}/../../dataset-creation/processManuscript.js",
):

    csl_processor_path = Path(csl_processor_path).resolve()
    if not csl_processor_path.is_file():
        print(f"{csl_processor_path} does not exist")
        exit(1)
    csl_processor_cwd = csl_processor_path.parent
    with subprocess.Popen(
        ["node", csl_processor_path, str(csl_http_port)],
        cwd=csl_processor_cwd,
    ) as p:
        print(f"CSL processor: {csl_processor_path}, pid: {p.pid}")
        time.sleep(1)
        styles: List[str] = styles_list(url=base_url(csl_http_port))
        len_styles = len(styles)
        print(f"CSL processor supports {len_styles} styles")

        db = DocBin(store_user_data=True)
        digests = set()
        for f in tqdm(crossref_files, desc=docbin_path.name):
            try:
                bibliographies = make_bibliography(
                    f,
                    randint(0, len_styles, NUM_STYLES_FOR_DOC).tolist(),
                    url=base_url(csl_http_port),
                )
                for bibliography in bibliographies:
                    if "references" not in bibliography:
                        # print("a problem for style ", bibliography["style"])
                        continue
                    references = bibliography["references"]
                    style = bibliography["style"]

                    # check digest if two styles produce the same string representation of the same crossref item
                    _references = []
                    for ref in references:
                        digest = hashlib.md5(ref.encode("utf-8")).digest()
                        if digest not in digests:
                            digests.add(digest)
                            _references.append(ref)
                        # else:
                        #     print("duplicate bib item: %s, style: %s", ref, style)
                    references = _references
                    if not references:
                        continue

                    docs = references_to_spacy(references, style, fname=f.name)

                    # downsample
                    if len(docs) // DOWNSAMPE_RATIO > 0:
                        docs = numpy.random.choice(
                            numpy.array(docs, dtype=numpy.dtype("object")),
                            len(docs) // DOWNSAMPE_RATIO,
                            replace=False,
                        )

                    for doc in docs:
                        if doc:
                            db.add(doc)
                        else:
                            log.warning("Can't parse a part of %s", f)
            except:
                log.exception("An exception while processing %s", f)

        p.terminate()
        db.to_disk(docbin_path)
    return docbin_path


def main(
    crossref_dir: Path,
    output_dir: Path = Path("train.ref"),
    parts: int = 100,
    parallel: int = 2,
    csl_processor_path=f"{Path(__file__).parent}/../../dataset-creation/processManuscript.js",
):
    """
    Sends crossref files to CSL processor and convert rendered annotated biblioraphies into Spacy DocBin files
    Dependency:
        node processManuscript.js
        needs to be started, for additional details see dataset-creation/README.md
    Args:
        crossref_dir: a directory with files downloaded from crossref, see dataset-creation/crossref/crossrefDownload.py
        output_dir: where to put *.docbin files
        parts: how many docbin files to be generated
        parallel: how manu processes to use. You might want to launch more
                  than one dataset-creation/processManuscript.js node process, i.e. in k8s cluster,
                  if you need high parallelism
    """
    Path(output_dir).mkdir(parents=True, exist_ok=True)

    input_files = [path for path in crossref_dir.glob("**/*.json")]
    input_files_parts = [
        arr.tolist() for arr in numpy.array_split(numpy.array(input_files), parts)
    ]
    futures: List[Future] = []
    if parallel > 0:
        with ProcessPoolExecutor(parallel) as e:
            for i, input_files_part in enumerate(input_files_parts):
                futures.append(
                    e.submit(
                        convert,
                        input_files_part,
                        output_dir / f"references.{i}.spacy",
                        3000 + i,
                        csl_processor_path,
                    )
                )
            for f in futures:
                print("convert:done:", f.result())
    else:
        # useful for debugging with ipython.embed()
        for i, input_files_part in enumerate(input_files_parts):
            result = convert(
                input_files_part,
                output_dir / f"references.{i}.spacy",
                3000 + i,
                csl_processor_path,
            )
            print("convert:done:", result)


if __name__ == "__main__":
    typer.run(main)
